{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":80874,"databundleVersionId":8794587,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install skimpy\n!pip install fastdtw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-23T09:59:03.608300Z","iopub.execute_input":"2024-07-23T09:59:03.608683Z","iopub.status.idle":"2024-07-23T10:00:05.628581Z","shell.execute_reply.started":"2024-07-23T09:59:03.608651Z","shell.execute_reply":"2024-07-23T10:00:05.627232Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting skimpy\n  Downloading skimpy-0.0.15-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: Pygments<3.0.0,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from skimpy) (2.17.2)\nRequirement already satisfied: click<9.0.0,>=8.1.6 in /opt/conda/lib/python3.10/site-packages (from skimpy) (8.1.7)\nRequirement already satisfied: ipykernel<7.0.0,>=6.7.0 in /opt/conda/lib/python3.10/site-packages (from skimpy) (6.28.0)\nRequirement already satisfied: numpy<2.0.0,>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from skimpy) (1.26.4)\nRequirement already satisfied: pandas<3.0.0,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from skimpy) (2.2.2)\nRequirement already satisfied: polars<0.21,>=0.19 in /opt/conda/lib/python3.10/site-packages (from skimpy) (0.20.31)\nRequirement already satisfied: pyarrow<17,>=13 in /opt/conda/lib/python3.10/site-packages (from skimpy) (16.1.0)\nRequirement already satisfied: rich<14.0,>=10.9 in /opt/conda/lib/python3.10/site-packages (from skimpy) (13.7.0)\nCollecting typeguard==4.2.1 (from skimpy)\n  Downloading typeguard-4.2.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting typing-extensions>=4.10.0 (from typeguard==4.2.1->skimpy)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (1.8.0)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (8.20.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (1.5.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel<7.0.0,>=6.7.0->skimpy) (5.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=2.0.3->skimpy) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=2.0.3->skimpy) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=2.0.3->skimpy) (2023.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0,>=10.9->skimpy) (3.0.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (3.0.42)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (4.8.0)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel<7.0.0,>=6.7.0->skimpy) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel<7.0.0,>=6.7.0->skimpy) (3.11.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0,>=10.9->skimpy) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.3->skimpy) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ipykernel<7.0.0,>=6.7.0->skimpy) (3.1.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel<7.0.0,>=6.7.0->skimpy) (0.2.2)\nDownloading skimpy-0.0.15-py3-none-any.whl (16 kB)\nDownloading typeguard-4.2.1-py3-none-any.whl (34 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nInstalling collected packages: typing-extensions, typeguard, skimpy\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.1.5\n    Uninstalling typeguard-4.1.5:\n      Successfully uninstalled typeguard-4.1.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed skimpy-0.0.15 typeguard-4.2.1 typing-extensions-4.12.2\nCollecting fastdtw\n  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastdtw) (1.26.4)\nBuilding wheels for collected packages: fastdtw\n  Building wheel for fastdtw (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=118275 sha256=d0a87106d90cbcb36108ed7c1a4cb24ef9c328d4ca963a5470f05280044141e3\n  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\nSuccessfully built fastdtw\nInstalling collected packages: fastdtw\nSuccessfully installed fastdtw-0.3.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport itertools\nimport warnings\nimport gc\nimport re\nimport scipy\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn import set_config\nfrom colorama import Style, Fore\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_predict, GroupKFold, TimeSeriesSplit\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom matplotlib.dates import MonthLocator, DateFormatter\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor, BaggingRegressor\nfrom lightgbm import LGBMRegressor\nfrom category_encoders import TargetEncoder, OneHotEncoder, MEstimateEncoder, OrdinalEncoder, CatBoostEncoder\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import roc_auc_score, roc_curve, make_scorer,mean_squared_error, mean_absolute_error, silhouette_score\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.dates as DateFormatter\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor, HistGradientBoostingClassifier\nimport math\nfrom fastdtw import fastdtw\nfrom sklearn.cluster       import AgglomerativeClustering\nimport skimpy\n\nsns.set_theme(style = 'white', palette = 'colorblind')\npal = sns.color_palette('colorblind')\n\npd.set_option('display.max_rows', 100)\nset_config(transform_output = 'pandas')\npd.options.mode.chained_assignment = None\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:22.98211Z","iopub.execute_input":"2024-06-22T02:29:22.982692Z","iopub.status.idle":"2024-06-22T02:29:27.877724Z","shell.execute_reply.started":"2024-06-22T02:29:22.982651Z","shell.execute_reply":"2024-06-22T02:29:27.876458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"SUBMIT = True","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:27.879349Z","iopub.execute_input":"2024-06-22T02:29:27.880054Z","iopub.status.idle":"2024-06-22T02:29:27.885352Z","shell.execute_reply.started":"2024-06-22T02:29:27.880018Z","shell.execute_reply":"2024-06-22T02:29:27.884056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printColor(pText: str):\n    print(f'{Style.BRIGHT}{Fore.GREEN}{pText}{Style.RESET_ALL}')    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:27.886684Z","iopub.execute_input":"2024-06-22T02:29:27.887053Z","iopub.status.idle":"2024-06-22T02:29:27.900719Z","shell.execute_reply.started":"2024-06-22T02:29:27.887007Z","shell.execute_reply":"2024-06-22T02:29:27.899438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/rohlik-orders-forecasting-challenge/train.csv',index_col='id')\ntrain_calendar = pd.read_csv(r'/kaggle/input/rohlik-orders-forecasting-challenge/train_calendar.csv')\ntest = pd.read_csv(r'/kaggle/input/rohlik-orders-forecasting-challenge/test.csv',index_col='id')\ntest_calendar = pd.read_csv(r'/kaggle/input/rohlik-orders-forecasting-challenge/test_calendar.csv')\nsub  = pd.read_csv(r'/kaggle/input/rohlik-orders-forecasting-challenge/solution_example.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:27.903509Z","iopub.execute_input":"2024-06-22T02:29:27.903914Z","iopub.status.idle":"2024-06-22T02:29:28.027504Z","shell.execute_reply.started":"2024-06-22T02:29:27.90388Z","shell.execute_reply":"2024-06-22T02:29:28.025906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.029233Z","iopub.execute_input":"2024-06-22T02:29:28.029612Z","iopub.status.idle":"2024-06-22T02:29:28.062041Z","shell.execute_reply.started":"2024-06-22T02:29:28.029581Z","shell.execute_reply":"2024-06-22T02:29:28.060976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.063937Z","iopub.execute_input":"2024-06-22T02:29:28.064285Z","iopub.status.idle":"2024-06-22T02:29:28.078266Z","shell.execute_reply.started":"2024-06-22T02:29:28.064256Z","shell.execute_reply":"2024-06-22T02:29:28.077008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_calendar.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.08026Z","iopub.execute_input":"2024-06-22T02:29:28.080645Z","iopub.status.idle":"2024-06-22T02:29:28.103785Z","shell.execute_reply.started":"2024-06-22T02:29:28.080615Z","shell.execute_reply":"2024-06-22T02:29:28.102512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Descriptive Statistics","metadata":{}},{"cell_type":"code","source":"print(f'{Style.BRIGHT}{Fore.YELLOW} SHAPE')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> train: {Fore.GREEN} {train.shape}')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> train_calendar: {Fore.GREEN} {train.shape}')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> test:  {Fore.GREEN} {test.shape}')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> test_calendar:  {Fore.GREEN} {test_calendar.shape}')\n\nprint(f'\\n\\n{Style.BRIGHT}{Fore.YELLOW} NULL VALUES')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> Train: {Fore.GREEN} {train.isnull().any().any()}')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> Test:  {Fore.GREEN} {test.isnull().any().any()}')\n\nprint(f'\\n\\n{Style.BRIGHT}{Fore.YELLOW} DUPLICATES')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> Train: {Fore.GREEN} {train.duplicated().any().any()}')\nprint(f'{Style.BRIGHT}{Fore.YELLOW} -> Test:  {Fore.GREEN} {test.duplicated().any().any()}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:28.105329Z","iopub.execute_input":"2024-06-22T02:29:28.105778Z","iopub.status.idle":"2024-06-22T02:29:28.135062Z","shell.execute_reply.started":"2024-06-22T02:29:28.105736Z","shell.execute_reply":"2024-06-22T02:29:28.133666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* For this first moment, only the features that we have in common between the training and test data will be used as a baseline.","metadata":{}},{"cell_type":"code","source":"initial_features = list(test.columns)\ntarget = 'orders'\nprint(f'Initial features: {Style.BRIGHT}{Fore.GREEN} {initial_features} {Style.RESET_ALL}')\nprint(f'target: {Style.BRIGHT}{Fore.BLUE}{target} {Style.RESET_ALL}')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.136791Z","iopub.execute_input":"2024-06-22T02:29:28.137186Z","iopub.status.idle":"2024-06-22T02:29:28.143796Z","shell.execute_reply.started":"2024-06-22T02:29:28.13715Z","shell.execute_reply":"2024-06-22T02:29:28.142491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skimpy.skim(train[initial_features+[target]])","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.14526Z","iopub.execute_input":"2024-06-22T02:29:28.145634Z","iopub.status.idle":"2024-06-22T02:29:28.279785Z","shell.execute_reply.started":"2024-06-22T02:29:28.145604Z","shell.execute_reply":"2024-06-22T02:29:28.278531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skimpy.skim(test)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.281504Z","iopub.execute_input":"2024-06-22T02:29:28.281884Z","iopub.status.idle":"2024-06-22T02:29:28.367675Z","shell.execute_reply.started":"2024-06-22T02:29:28.281848Z","shell.execute_reply":"2024-06-22T02:29:28.366308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time (temporal feature)","metadata":{}},{"cell_type":"code","source":"for df in [train, test]:\n    df['date'] = pd.to_datetime(df['date'])\n    df['holiday_name'] = df['holiday_name'].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.36953Z","iopub.execute_input":"2024-06-22T02:29:28.369958Z","iopub.status.idle":"2024-06-22T02:29:28.388134Z","shell.execute_reply.started":"2024-06-22T02:29:28.369925Z","shell.execute_reply":"2024-06-22T02:29:28.386646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = train['date'].copy()\nfreq = freq.to_frame().reset_index()\nfreq.set_index('date',inplace=True)\nprint(f'Frequency days: {pd.Series(freq.index.max()-freq.index.min()).value_counts()[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.392976Z","iopub.execute_input":"2024-06-22T02:29:28.393723Z","iopub.status.idle":"2024-06-22T02:29:28.410138Z","shell.execute_reply.started":"2024-06-22T02:29:28.393688Z","shell.execute_reply":"2024-06-22T02:29:28.408991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train')\nprint(f'first date (train) : {train.date.min()}')\nprint(f'last date (train) : {train.date.max()}')\nprint(f'interval (train) : {train.date.max()-train.date.min()}')\nprint(50*'*')\nprint('\\nTest')\nprint(f'first date (test) : {test.date.min()}')\nprint(f'last date (test) : {test.date.max()}')\nprint(f'interval (test) : {test.date.max()-test.date.min()}')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.411618Z","iopub.execute_input":"2024-06-22T02:29:28.412311Z","iopub.status.idle":"2024-06-22T02:29:28.426928Z","shell.execute_reply.started":"2024-06-22T02:29:28.412277Z","shell.execute_reply":"2024-06-22T02:29:28.425605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for  w in train.warehouse.unique():\n    print(f'{Style.BRIGHT}{Fore.BLUE}**{w}**{Style.RESET_ALL}')\n    print(f'  First date: {train.loc[train.warehouse==w].date.min().strftime(\"%Y-%m-%d\")}')\n    print(f'  Last  date: {train.loc[train.warehouse==w].date.max().strftime(\"%Y-%m-%d\")}')        \n    print(f'{Style.BRIGHT}{Fore.YELLOW} Missing Dates-> {Style.RESET_ALL}{pd.date_range(start=train.loc[train.warehouse==w].date.min(),end=train.loc[train.warehouse==w].date.max()).difference(train.loc[train.warehouse==w].date)}\\n')\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:28.428651Z","iopub.execute_input":"2024-06-22T02:29:28.429031Z","iopub.status.idle":"2024-06-22T02:29:28.511957Z","shell.execute_reply.started":"2024-06-22T02:29:28.428999Z","shell.execute_reply":"2024-06-22T02:29:28.510612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we can see, there are series with different periods and also some with data without records, some we know refer to holidays, such as 12/25. Brno_1, Budapest_1, Prague_1, Prague_2 and Prague_3, have their series starting on 2020-12-05 and going until 2024-03-15, while Munich_1 starts on 2021-07-21 and ends on 2024-03-15. Frankfurt_1 starts in 2022. Perhaps we can split these series during our cross-validation.\n* Let's try below to find a pattern in the warehouse series, through clustering.","metadata":{}},{"cell_type":"markdown","source":"# Clustering","metadata":{}},{"cell_type":"markdown","source":"* First, a distance matrix between the time series of orders in different warehouses is calculated using a fast version of Dynamic Time Warping (DTW)","metadata":{}},{"cell_type":"code","source":"%%time\ndc = train.copy()\nlist_wh = train.warehouse.unique()\ndim_matrix =  dc['warehouse'].drop_duplicates().shape[0]\ndistance_matrix = np.zeros((dim_matrix,dim_matrix))\nfor i,v1 in enumerate(list_wh):\n    w1 = dc[dc.warehouse==v1]['orders'].values\n    for j,v2 in enumerate(list_wh):\n        w2 = dc[dc.warehouse==v2]['orders'].values\n    \n        distance, path = fastdtw(w1,w2)\n        distance_matrix[i][j] = distance","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.513543Z","iopub.execute_input":"2024-06-22T02:29:28.513934Z","iopub.status.idle":"2024-06-22T02:29:28.648584Z","shell.execute_reply.started":"2024-06-22T02:29:28.513902Z","shell.execute_reply":"2024-06-22T02:29:28.647289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we try to find the number of ideal clusters, using the Silhouette score.\n* The Silhouette Score can be used to determine the optimal number of clusters in a clustering analysis, as it provides a measure of how compact and well separated the clusters are. The higher the average Silhouette Score for a given number of clusters, the better the separation between the clusters.","metadata":{}},{"cell_type":"code","source":"%%time\nmax_clusters = 6\nclusters = np.arange(2,max_clusters+1)\nsilhouttes = []\nfor c in clusters:\n    model = AgglomerativeClustering(affinity='precomputed', n_clusters=c, linkage='complete').fit(distance_matrix)    \n    s = silhouette_score(distance_matrix, model.fit_predict(distance_matrix))\n    silhouttes.append(s)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:28.650165Z","iopub.execute_input":"2024-06-22T02:29:28.650624Z","iopub.status.idle":"2024-06-22T02:29:28.675958Z","shell.execute_reply.started":"2024-06-22T02:29:28.650585Z","shell.execute_reply":"2024-06-22T02:29:28.674554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Agglomerative Clustering is a hierarchical clustering algorithm used to group data. It is a bottom-up approach, where each data point starts as its own cluster and then clusters are iteratively merged based on their proximity, forming a hierarchy of clusters. documentation link: [AgglomerativeClustering_SkLearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,3))\nplt.plot(clusters,silhouttes)\nplt.grid(True,lw=0.5,ls='--')\nplt.ylabel('Silhouette score')\nplt.xlabel('number clusters')\nplt.title('Clusters',fontweight='bold');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:28.677294Z","iopub.execute_input":"2024-06-22T02:29:28.677675Z","iopub.status.idle":"2024-06-22T02:29:29.074994Z","shell.execute_reply.started":"2024-06-22T02:29:28.677633Z","shell.execute_reply":"2024-06-22T02:29:29.073507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Above we can see that the ideal number of clusters is 2.","metadata":{}},{"cell_type":"code","source":"model = AgglomerativeClustering(affinity='precomputed',\n                                n_clusters=2,\n                                linkage='complete').fit(distance_matrix)\nclusters = model.labels_\nc = dict(zip(list_wh,clusters))\ndc['clusters'] = dc['warehouse'].copy()\ndc['clusters'] = dc['clusters'].map(c)\ndisplay(dc[['warehouse', 'clusters']].drop_duplicates().groupby('clusters').count().reset_index())\ndisplay(pd.DataFrame(list(c.items()), columns=['warehouse', 'cluster']).style.background_gradient(axis=0))\n\ndel dc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:29.077164Z","iopub.execute_input":"2024-06-22T02:29:29.077629Z","iopub.status.idle":"2024-06-22T02:29:29.296709Z","shell.execute_reply.started":"2024-06-22T02:29:29.077591Z","shell.execute_reply":"2024-06-22T02:29:29.295592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we can see, from the results Prague_1 and Brno_1 belong to the same cluster, and the others to another. We can train 2 models, one for each cluster.","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:53:59.834916Z","iopub.execute_input":"2024-06-14T02:53:59.835348Z","iopub.status.idle":"2024-06-14T02:53:59.847108Z","shell.execute_reply.started":"2024-06-14T02:53:59.835314Z","shell.execute_reply":"2024-06-14T02:53:59.845952Z"}}},{"cell_type":"markdown","source":"# Target","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(7,3, figsize=(14,7*3), gridspec_kw={'width_ratios':[0.70, 0.30, 0.20 ]})\n\nfor i,(ind, c) in enumerate(train.groupby('warehouse')):   \n    ax = axes[i,0]\n    sns.lineplot(data=c,x=c.date,y='orders',ax=ax)\n    ax.set_title(f'{ind}')\n    \n    ax = axes[i,1]\n    sns.histplot(data=c,x='orders',ax=ax,color='green')               \n    ax.set_title(f'{ind} - skew ({c.orders.skew().round(2)})')\n    \n    ax = axes[i,2]\n    sns.boxplot(data=c,y='orders',ax=ax,color='orange', linewidth=.75, fliersize=2.01, width=0.80)               \n    \nplt.tight_layout()    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:29.29836Z","iopub.execute_input":"2024-06-22T02:29:29.298741Z","iopub.status.idle":"2024-06-22T02:29:38.591663Z","shell.execute_reply.started":"2024-06-22T02:29:29.298711Z","shell.execute_reply":"2024-06-22T02:29:38.59043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,3))\nsns.histplot(train['orders']);\nplt.title('Target distribuition',fontweight='bold');","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:38.593013Z","iopub.execute_input":"2024-06-22T02:29:38.593352Z","iopub.status.idle":"2024-06-22T02:29:39.121325Z","shell.execute_reply.started":"2024-06-22T02:29:38.593324Z","shell.execute_reply":"2024-06-22T02:29:39.119921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(figsize=(11,6))\nsns.lineplot(data=train,x='date',\n             y='orders',\n             errorbar=None, \n             hue='warehouse',\n             palette='tab10',ax=ax);\nax.legend(bbox_to_anchor=[1, 1]);\nax.set_title('Distribuitions of Orders by warehouse');","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:39.12292Z","iopub.execute_input":"2024-06-22T02:29:39.1233Z","iopub.status.idle":"2024-06-22T02:29:40.216977Z","shell.execute_reply.started":"2024-06-22T02:29:39.123267Z","shell.execute_reply":"2024-06-22T02:29:40.215521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trend","metadata":{}},{"cell_type":"markdown","source":"First degree polynomial (one line) fitting is often used to model linear trends in time series data.","metadata":{}},{"cell_type":"code","source":"colors = sns.color_palette()\nfig, axes = plt.subplots(7,1, figsize=(9,4*3))\nax = axes.flatten()\nfor i,(ind, c) in enumerate(train.groupby('warehouse')):  \n    temp = c.groupby(c.date).orders.mean()\n    poly = np.polynomial.polynomial.Polynomial.fit(range(len(temp.index)), temp, deg=1)\n    ax[i].plot(temp.index, poly(range(len(temp.index))),color=colors[i])\n    ax[i].set_title(f'Trend - {ind}')\n    \nplt.tight_layout()    ","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:40.218688Z","iopub.execute_input":"2024-06-22T02:29:40.21913Z","iopub.status.idle":"2024-06-22T02:29:45.399757Z","shell.execute_reply.started":"2024-06-22T02:29:40.219093Z","shell.execute_reply":"2024-06-22T02:29:45.398162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Above we have the graphs of time series trends of average orders over time for different warehouses. First degree polynomial fitting allows you to visualize the general direction of trends.","metadata":{}},{"cell_type":"code","source":"all_orders = train.copy()\nall_orders.reset_index(inplace=True)\nall_orders = all_orders.set_index('date')\nall_orders = all_orders.set_index(['warehouse'],append=True)\naverage_orders = all_orders.groupby('date')['orders'].mean()\nax= average_orders.plot(figsize=(10,4),\n                        markeredgecolor='#9F2B68',\n                        #markerfacecolor='#9F2B68',\n                        style='.-',\n                        color='0.8')\n\nstart_date = average_orders.index.min()\nend_date = average_orders.index.max()\nsmallest_orders = average_orders.nsmallest(8)\nlargest_orders = average_orders.nlargest(10)\ndates_to_plot = pd.date_range(start=start_date, end=end_date, freq='1M')\n# for date in dates_to_plot:\n#     if date.month == 12:\n#         ax.axvline(date, color='red', linestyle='--', linewidth=0.7)\n        \nsmallest_scatter = ax.scatter(smallest_orders.index, \n                              smallest_orders.values, \n                              color='red', \n                              label='8 Smallest Orders')\n\nlegend_labels = []\nfor date in smallest_orders.index:\n    legend_labels.append(f'{date.strftime(\"%Y-%m\")}: {smallest_orders[date]}')\n     \nax.set_xticks(dates_to_plot)\nax.set_xticklabels(dates_to_plot.strftime('%Y-%m'), rotation=90)\nax.set_title('Average orders represent the general orders of all warehouses');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:35:00.209563Z","iopub.execute_input":"2024-06-22T02:35:00.210005Z","iopub.status.idle":"2024-06-22T02:35:01.026769Z","shell.execute_reply.started":"2024-06-22T02:35:00.20997Z","shell.execute_reply":"2024-06-22T02:35:01.02535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The graph shows a general trend of increasing average orders over time, with some seasonal fluctuations.\n* some seasonal fluctuations in average orders. For example, there appears to be a spike in average orders at the end of the year","metadata":{}},{"cell_type":"markdown","source":"# Initial features","metadata":{}},{"cell_type":"code","source":"cat_features      = ['warehouse','holiday_name']\nnum_features      = ['holiday','shops_closed','winter_school_holidays']\ntemporal_features = ['date']\n\nfor df in [train,test]:\n    df[cat_features] = df[cat_features].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:46.262247Z","iopub.execute_input":"2024-06-22T02:29:46.262627Z","iopub.status.idle":"2024-06-22T02:29:46.280337Z","shell.execute_reply.started":"2024-06-22T02:29:46.262594Z","shell.execute_reply":"2024-06-22T02:29:46.278842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Some models require the type of categorical variable to be of the Category type (for example LGBM and XGBoost), in addition the category type is more efficient than the 'object'","metadata":{}},{"cell_type":"markdown","source":"# Feature distributions","metadata":{}},{"cell_type":"code","source":"def plot_numerical():   \n    df = pd.concat([train[num_features].assign(Source = 'Train'), \n                    test[num_features].assign(Source = 'Test')], ignore_index = True)\n    \n    fig, axes = plt.subplots(len(num_features), 1 ,figsize = (5, len(num_features) * 2), \n                             gridspec_kw = {'hspace': 0.55, 'wspace': 0})\n    axes = axes.flatten()\n\n    for i,col in enumerate(num_features):\n        ax = axes[i]\n        sns.histplot(data = df[[col, 'Source']], x = col, hue = 'Source', palette=['#456cf0', '#ed7647'],ax = ax) \n        ax.set_title(f\"\\n{col}\",fontsize = 9)\n        ax.grid(visible=True, which = 'both', linestyle = '--', color='lightgrey', linewidth = 0.75)\n        ax.set(xlabel = '', ylabel = '')\n\n    plt.suptitle(f'\\nDistribution analysis - numerical features',fontsize = 12, y = 1, x = 0.57, fontweight='bold')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:46.281887Z","iopub.execute_input":"2024-06-22T02:29:46.282397Z","iopub.status.idle":"2024-06-22T02:29:46.29213Z","shell.execute_reply.started":"2024-06-22T02:29:46.282355Z","shell.execute_reply":"2024-06-22T02:29:46.290918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_numerical()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:46.293818Z","iopub.execute_input":"2024-06-22T02:29:46.294641Z","iopub.status.idle":"2024-06-22T02:29:47.344693Z","shell.execute_reply.started":"2024-06-22T02:29:46.2946Z","shell.execute_reply":"2024-06-22T02:29:47.34333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* For all distributions with positive skewness and below zero, applying a logarithmic transformation can help bring them closer to normal. This contributes to some machine learning models.","metadata":{}},{"cell_type":"code","source":"_,ax = plt.subplots(2,1, figsize=(7,8), gridspec_kw={'hspace':0.70})\nax =ax.flatten()\n\nsns.countplot(data=train, x='warehouse',ax=ax[0])\nax[0].set_xticklabels(labels=ax[0].get_xticklabels(),rotation=45);\nvc = train['warehouse'].cat.codes.value_counts() / len(train)\nax[0].bar_label(ax[0].containers[0], label_type='center');\nax[0].set_title('Train - Warehouse')\nax[0].set_xlabel('')\n\nsns.countplot(data=test, x='warehouse',ax=ax[1])\nax[1].set_xticklabels(labels=ax[1].get_xticklabels(),rotation=45);\nvc = train['warehouse'].cat.codes.value_counts() / len(train)\nax[1].bar_label(ax[1].containers[0], label_type='center');\nax[1].set_title('Test - Warehouse')\nax[1].set_xlabel('');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-22T02:29:47.346613Z","iopub.execute_input":"2024-06-22T02:29:47.347539Z","iopub.status.idle":"2024-06-22T02:29:48.069377Z","shell.execute_reply.started":"2024-06-22T02:29:47.347494Z","shell.execute_reply":"2024-06-22T02:29:48.068087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation","metadata":{}},{"cell_type":"code","source":"corr_features = [f for f in initial_features+[target] if f not in ['date','id']+cat_features]\ncc = np.corrcoef(train[corr_features], rowvar=False)\nmask = np.zeros_like(cc)\nmask[np.triu_indices_from(mask)]=True\nplt.figure(figsize=(4, 3))\nsns.heatmap(cc*10, center=0, cmap='coolwarm', annot=True, fmt='.0f',\n            xticklabels=corr_features, yticklabels=corr_features,mask=mask)\nplt.title('Correlation',fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.070763Z","iopub.execute_input":"2024-06-22T02:29:48.071115Z","iopub.status.idle":"2024-06-22T02:29:48.40666Z","shell.execute_reply.started":"2024-06-22T02:29:48.071084Z","shell.execute_reply":"2024-06-22T02:29:48.405322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* holiday is correlated with shops_closed.","metadata":{}},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, cols):\n        self.cols = cols\n        \n    def fit(self, x, y=None):\n        return self\n    \n    def transform(self, x):\n        return x[self.cols]","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.408081Z","iopub.execute_input":"2024-06-22T02:29:48.408457Z","iopub.status.idle":"2024-06-22T02:29:48.415474Z","shell.execute_reply.started":"2024-06-22T02:29:48.408427Z","shell.execute_reply":"2024-06-22T02:29:48.414006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DateProcessor(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self, X):\n        x_copy = X.copy()\n        x_copy['year'] = x_copy['date'].dt.year\n        x_copy['month'] = x_copy['date'].dt.month\n        x_copy['day']   = x_copy['date'].dt.day\n        x_copy['dayofweek'] = x_copy['date'].dt.dayofweek\n        x_copy['sin_month']=np.sin(2*np.pi*x_copy['month']/12)\n        x_copy['cos_month']=np.cos(2*np.pi*x_copy['month']/12)\n        x_copy['sin_day']=np.sin(2*np.pi*x_copy['day']/30)\n        x_copy['cos_day']=np.cos(2*np.pi*x_copy['month']/30)\n        \n        return x_copy.drop('date',axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.417058Z","iopub.execute_input":"2024-06-22T02:29:48.417506Z","iopub.status.idle":"2024-06-22T02:29:48.433641Z","shell.execute_reply.started":"2024-06-22T02:29:48.417466Z","shell.execute_reply":"2024-06-22T02:29:48.432506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross validation","metadata":{}},{"cell_type":"code","source":"oof, scores, test_preds   = {}, {}, {}   ","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.435407Z","iopub.execute_input":"2024-06-22T02:29:48.435764Z","iopub.status.idle":"2024-06-22T02:29:48.448169Z","shell.execute_reply.started":"2024-06-22T02:29:48.435736Z","shell.execute_reply":"2024-06-22T02:29:48.446837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"def MAPE(y_true,y_pred):\n    return np.mean(np.abs(y_pred-y_true)/y_true)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.449496Z","iopub.execute_input":"2024-06-22T02:29:48.44989Z","iopub.status.idle":"2024-06-22T02:29:48.464981Z","shell.execute_reply.started":"2024-06-22T02:29:48.44985Z","shell.execute_reply":"2024-06-22T02:29:48.463367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = GroupKFold(n_splits=5)\ntss = TimeSeriesSplit(n_splits=5)\n\ndef cross_validate(model, label=''):\n    \n    if label is None:\n        label = type(model).__name__\n        \n    X = train.copy()\n    y = X.pop('orders')\n    \n    val_scores_mape, val_scores_mae, dates = [], [], [],    \n    \n    y_preds = np.zeros((len(train)))\n    for i, (trx_idx, val_idx) in enumerate(tss.split(X,y,groups=X['warehouse'])):\n        X_train = X.iloc[trx_idx]\n        y_train = y.iloc[trx_idx]\n                \n        X_val   = X.iloc[val_idx]\n        y_val   = y.iloc[val_idx]            \n    \n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_val)\n        y_preds[val_idx] += y_pred\n        mape = MAPE(y_val,y_pred)\n        val_scores_mape.append(mape)\n        val_scores_mae.append(mean_absolute_error(y_val,y_pred))        \n        \n        print(f'Fold {i+1}: MAPE: {mape} | MAE: {mean_absolute_error(y_val,y_pred)} ')\n    \n    \n    oof[label] = y_preds\n    scores[label] = np.mean(val_scores_mape)\n    printColor(f'MAPE: {np.mean(val_scores_mape)} +- {np.std(val_scores_mape)} | MAE: {np.mean(val_scores_mae)} +- {np.std(val_scores_mae)} | {label}')     \n        \n   \n    if SUBMIT:\n        X = train.copy()\n        y = X.pop('orders')\n        y_preds = np.zeros((len(test)))\n        model.fit(X,y)\n        y_preds += model.predict(test)\n    test_preds[label] = y_preds   \n         ","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.466928Z","iopub.execute_input":"2024-06-22T02:29:48.467429Z","iopub.status.idle":"2024-06-22T02:29:48.481801Z","shell.execute_reply.started":"2024-06-22T02:29:48.467389Z","shell.execute_reply":"2024-06-22T02:29:48.48048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_validate(make_pipeline(FeatureSelector(initial_features),\n                             DateProcessor(),\n                             MEstimateEncoder(cols=cat_features),\n                             TransformedTargetRegressor(\n                                          XGBRegressor(enable_categorical=False,\n                                                       random_state=42,                                                                                                              \n                                                       n_estimators=100),\n                                          func=np.log1p,\n                                          inverse_func=np.expm1)\n                                          ),'XGB')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:48.48335Z","iopub.execute_input":"2024-06-22T02:29:48.483715Z","iopub.status.idle":"2024-06-22T02:29:49.888474Z","shell.execute_reply.started":"2024-06-22T02:29:48.483684Z","shell.execute_reply":"2024-06-22T02:29:49.887459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'verbose': -1,'random_state': 42}\ncross_validate(make_pipeline(FeatureSelector(initial_features),\n                             DateProcessor(),\n                             OrdinalEncoder(cols='holiday_name'),\n                             TransformedTargetRegressor(\n                                          LGBMRegressor(**params),\n                                          func=np.log1p,\n                                          inverse_func=np.expm1)\n                                          ),'LGBM')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:49.889918Z","iopub.execute_input":"2024-06-22T02:29:49.890983Z","iopub.status.idle":"2024-06-22T02:29:51.105256Z","shell.execute_reply.started":"2024-06-22T02:29:49.890948Z","shell.execute_reply":"2024-06-22T02:29:51.103734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_validate(make_pipeline(FeatureSelector(initial_features),\n                             DateProcessor(),\n                             ColumnTransformer([('ohe', OneHotEncoder(), cat_features)],remainder='passthrough'),                            \n                             TransformedTargetRegressor(\n                                          RandomForestRegressor(\n                                                       random_state=42,\n                                                       verbose=0,\n                                                       max_depth=9),\n                                          func=np.log1p,\n                                          inverse_func=np.expm1)\n                                          ),'RF')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:51.106945Z","iopub.execute_input":"2024-06-22T02:29:51.107395Z","iopub.status.idle":"2024-06-22T02:29:57.181716Z","shell.execute_reply.started":"2024-06-22T02:29:51.107363Z","shell.execute_reply":"2024-06-22T02:29:57.180421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor label in oof.keys():\n    score = MAPE(train['orders'], oof[label])\n    result.append((label, score))\nresult_df = pd.DataFrame(result, columns=['label', 'score'])\nresult_df.sort_values('score', inplace=True, ascending=False)\nresult_df","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:57.183496Z","iopub.execute_input":"2024-06-22T02:29:57.183875Z","iopub.status.idle":"2024-06-22T02:29:57.201777Z","shell.execute_reply.started":"2024-06-22T02:29:57.183844Z","shell.execute_reply":"2024-06-22T02:29:57.200181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"if SUBMIT:\n    sub['orders'] = test_preds['LGBM']*1.01\n    sub['orders'].hist(figsize=(5,3))\n    sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:29:57.203277Z","iopub.execute_input":"2024-06-22T02:29:57.203643Z","iopub.status.idle":"2024-06-22T02:29:57.558705Z","shell.execute_reply.started":"2024-06-22T02:29:57.203615Z","shell.execute_reply":"2024-06-22T02:29:57.557606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* LGBM obtained better results in this first stage, and also obtained a lower standard deviation in relation to the others.\n* For better results, you must: tune the LGBM hyperparameters, and adapt the cross-validation, so that we can leave the results correlated with the LB.\n* An ensemble can be valuable.\n* train different models for each cluster found.","metadata":{}}]}